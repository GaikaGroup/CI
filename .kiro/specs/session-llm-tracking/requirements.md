# Requirements Document

## Introduction

This specification describes the implementation of LLM model tracking for chat sessions. The system needs to store information about which LLM model was used to generate each assistant response within a session. This enables analytics, debugging, model comparison, and provides transparency to users about which AI models are being used in their conversations.

## Glossary

- **Session**: A conversation thread between a user and the AI assistant
- **Message**: A single message within a session (either from user or assistant)
- **LLM (Large Language Model)**: The AI model used to generate assistant responses (e.g., GPT-4, GPT-3.5-turbo, Llama-3, etc.)
- **Model Metadata**: Information about the LLM including provider, model name, version, and parameters
- **Assistant Message**: A message generated by the AI system in response to user input
- **Provider**: The service providing the LLM (e.g., OpenAI, Ollama, Anthropic)

## Requirements

### Requirement 1: Store LLM Model Information for Each Assistant Message

**User Story:** As a system administrator, I want to track which LLM model generated each assistant response, so that I can analyze model usage patterns and debug issues.

#### Acceptance Criteria

1. WHEN THE System generates an assistant response, THE System SHALL store the LLM model identifier in the message metadata
2. WHEN THE System stores model information, THE System SHALL include the provider name (e.g., "openai", "ollama")
3. WHEN THE System stores model information, THE System SHALL include the model name (e.g., "gpt-4", "llama-3-8b")
4. WHEN THE System stores model information, THE System SHALL include the model version if available
5. WHEN THE System stores model information, THE System SHALL include a timestamp of when the model was used

### Requirement 2: Support Multiple Models Within a Single Session

**User Story:** As a user, I want the system to track when different LLM models are used within the same conversation, so that I can understand which model generated which response.

#### Acceptance Criteria

1. WHEN different LLM models are used in a session, THE System SHALL store each model's information separately for each message
2. WHEN THE System switches between models mid-session, THE System SHALL maintain accurate tracking without data loss
3. WHEN THE System retrieves session history, THE System SHALL include model information for each assistant message
4. WHEN a session contains messages from multiple models, THE System SHALL preserve the chronological order with correct model attribution

### Requirement 3: Extend Message Metadata Schema

**User Story:** As a developer, I want a well-defined schema for storing LLM model information in message metadata, so that the data is consistent and easy to query.

#### Acceptance Criteria

1. WHEN THE System stores model information, THE System SHALL use a consistent JSON schema in the message metadata field
2. WHEN THE System stores model information, THE System SHALL include fields for provider, model, version, and timestamp
3. WHEN THE System stores model information, THE System SHALL support optional fields for temperature, max_tokens, and other parameters
4. WHEN THE System validates message metadata, THE System SHALL ensure required model fields are present for assistant messages
5. WHEN THE System stores model information, THE System SHALL handle missing or null values gracefully

### Requirement 4: Retrieve Model Usage Statistics

**User Story:** As a system administrator, I want to query which models were used in a session, so that I can generate usage reports and analyze model performance.

#### Acceptance Criteria

1. WHEN THE System queries a session, THE System SHALL provide a list of all unique models used in that session
2. WHEN THE System generates statistics, THE System SHALL count how many messages were generated by each model
3. WHEN THE System retrieves session details, THE System SHALL include model information in the response
4. WHEN THE System aggregates data across sessions, THE System SHALL support filtering by model provider and name
5. WHEN THE System calculates usage metrics, THE System SHALL include timestamps for temporal analysis

### Requirement 5: Hide Model Information from Regular Users

**User Story:** As a product manager, I want to hide LLM model information from regular users, so that they focus on content quality rather than technical details.

#### Acceptance Criteria

1. WHEN THE System displays an assistant message to a regular user, THE System SHALL not show the model name
2. WHEN THE System displays an assistant message to a regular user, THE System SHALL not show model configuration parameters
3. WHEN THE System displays a session to a regular user, THE System SHALL not provide options to view model information
4. WHEN a regular user accesses the API, THE System SHALL not include model information in the response
5. WHEN THE System displays chat history to a regular user, THE System SHALL hide all model-related metadata

### Requirement 6: User Feedback System with Dislike Button

**User Story:** As a user, I want to provide feedback on assistant responses that I don't like, so that the system can improve over time.

#### Acceptance Criteria

1. WHEN THE System displays an assistant message, THE System SHALL show a dislike icon button
2. WHEN the user clicks the dislike button, THE System SHALL open a dialog window
3. WHEN the dialog opens, THE System SHALL prompt the user to write their feedback
4. WHEN the user submits feedback, THE System SHALL store the feedback text with the message
5. WHEN the user submits feedback, THE System SHALL associate the feedback with the LLM model that generated the response

### Requirement 7: Support Model Fallback Tracking

**User Story:** As a developer, I want to track when the system falls back from one model to another, so that I can monitor system reliability and model availability.

#### Acceptance Criteria

1. WHEN THE System attempts to use a primary model and fails, THE System SHALL record the attempted model
2. WHEN THE System falls back to an alternative model, THE System SHALL store both the attempted and actual model used
3. WHEN THE System stores fallback information, THE System SHALL include the reason for fallback
4. WHEN THE System retrieves message history, THE System SHALL indicate which messages used fallback models
5. WHEN THE System generates reports, THE System SHALL calculate fallback rates per model

### Requirement 8: Maintain Backward Compatibility

**User Story:** As a system administrator, I want existing sessions without model tracking to continue working, so that the system upgrade is seamless.

#### Acceptance Criteria

1. WHEN THE System retrieves old messages without model metadata, THE System SHALL handle them gracefully without errors
2. WHEN THE System displays old messages, THE System SHALL show a default indicator for unknown models
3. WHEN THE System queries sessions, THE System SHALL support both old and new message formats
4. WHEN THE System migrates data, THE System SHALL preserve all existing message content and metadata
5. WHEN THE System processes old sessions, THE System SHALL not require model information for historical data

### Requirement 9: Store User Feedback with Message

**User Story:** As a system, I want to store user feedback along with message and model information, so that administrators can analyze which models receive negative feedback.

#### Acceptance Criteria

1. WHEN the user submits feedback, THE System SHALL store the feedback text in the message metadata
2. WHEN the user submits feedback, THE System SHALL store the timestamp of when feedback was submitted
3. WHEN the user submits feedback, THE System SHALL mark the message as having user feedback
4. WHEN the user submits feedback, THE System SHALL associate it with the LLM model information
5. WHEN the user submits feedback, THE System SHALL allow only one feedback per message per user

### Requirement 10: Administrator Interface for Feedback Review

**User Story:** As an administrator, I want to view user feedback along with model information, so that I can identify problematic responses and improve the system.

#### Acceptance Criteria

1. WHEN the administrator views a session, THE System SHALL display which messages have user feedback
2. WHEN the administrator views a message with feedback, THE System SHALL show the feedback text
3. WHEN the administrator views a message with feedback, THE System SHALL show which LLM model generated the response
4. WHEN the administrator views a message with feedback, THE System SHALL show the model configuration parameters
5. WHEN the administrator views feedback, THE System SHALL show the timestamp of when feedback was submitted

### Requirement 11: Store Model Configuration Parameters

**User Story:** As a developer, I want to store the configuration parameters used for each model invocation, so that administrators can understand the context of user feedback.

#### Acceptance Criteria

1. WHEN THE System invokes an LLM, THE System SHALL store the temperature parameter used
2. WHEN THE System invokes an LLM, THE System SHALL store the max_tokens parameter used
3. WHEN THE System invokes an LLM, THE System SHALL store any custom system prompts or instructions
4. WHEN THE System stores configuration, THE System SHALL include the timestamp of invocation
5. WHEN THE System retrieves model information, THE System SHALL include all stored configuration parameters

### Requirement 12: Display Feedback Statistics on Analytics Page

**User Story:** As an administrator, I want to see feedback statistics on the analytics page at /stats, so that I can monitor overall system quality.

#### Acceptance Criteria

1. WHEN the administrator accesses /stats page, THE System SHALL display the total count of dislikes
2. WHEN the administrator views /stats page, THE System SHALL show dislike statistics grouped by LLM model
3. WHEN the administrator views /stats page, THE System SHALL show dislike trends over time
4. WHEN the administrator views /stats page, THE System SHALL show the percentage of messages with dislikes
5. WHEN the administrator views /stats page, THE System SHALL provide a link to detailed feedback view

### Requirement 13: Display Detailed Feedback on Admin Sessions Page

**User Story:** As an administrator, I want to see detailed user feedback when viewing sessions at /admin/sessions, so that I can understand specific issues.

#### Acceptance Criteria

1. WHEN the administrator views a session at /admin/sessions, THE System SHALL highlight messages with user feedback
2. WHEN the administrator views a message with feedback, THE System SHALL display the dislike icon indicator
3. WHEN the administrator views a message with feedback, THE System SHALL show the full feedback text
4. WHEN the administrator views a message with feedback, THE System SHALL show which LLM model generated the response
5. WHEN the administrator views a message with feedback, THE System SHALL show the timestamp of when feedback was submitted

### Requirement 14: Ensure Data Privacy and Security

**User Story:** As a security officer, I want to ensure that model tracking and feedback does not expose sensitive information, so that user privacy is maintained.

#### Acceptance Criteria

1. WHEN THE System stores model information, THE System SHALL not include API keys or credentials
2. WHEN THE System stores model information, THE System SHALL not include personally identifiable information
3. WHEN THE System exposes model information via API, THE System SHALL enforce administrator authentication
4. WHEN THE System logs model usage, THE System SHALL comply with data retention policies
5. WHEN THE System displays feedback to administrators, THE System SHALL enforce role-based access control
