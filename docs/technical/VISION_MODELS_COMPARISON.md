# Сравнение Vision моделей для Ollama

## Проблема с llava:7b

- Очень медленная (6+ минут на ответ)
- Требует много памяти
- Долго загружается

## Альтернативные модели

### 1. **minicpm-v** (РЕКОМЕНДУЕТСЯ)

```bash
ollama pull minicpm-v:8b
```

**Преимущества:**

- Быстрее llava в 3-4 раза
- Лучше понимает текст на изображениях
- Меньше памяти (8 GB достаточно)
- Поддерживает русский язык

**Размер:** ~5 GB

### 2. **bakllava**

```bash
ollama pull bakllava
```

**Преимущества:**

- Быстрее llava:7b
- Хорошо работает с текстом
- Меньше требований к памяти

**Размер:** ~4.4 GB

### 3. **llava:13b** (если есть память)

```bash
ollama pull llava:13b
```

**Преимущества:**

- Лучше качество, чем 7b
- Но все равно медленная

**Размер:** ~7.4 GB
**Требования:** 16+ GB RAM

### 4. **llava-phi3** (самая быстрая)

```bash
ollama pull llava-phi3
```

**Преимущества:**

- Самая быстрая из всех
- Маленький размер
- Хорошо для простых задач

**Размер:** ~2.9 GB
**Недостатки:** Хуже качество для сложных изображений

## Как изменить модель

### Вариант 1: Через .env

```bash
# В файле .env или .env.local
VITE_OLLAMA_VISION_MODEL=minicpm-v:8b
```

### Вариант 2: Напрямую в конфиге

Отредактируйте `src/lib/config/llm.js`:

```javascript
VISION_MODEL: import.meta.env.VITE_OLLAMA_VISION_MODEL || 'minicpm-v:8b',
```

## Тест производительности

### llava:7b

- Загрузка: ~1 минута
- Ответ: 6+ минут
- Память: ~6 GB

### minicpm-v:8b (ожидаемо)

- Загрузка: ~20 секунд
- Ответ: 1-2 минуты
- Память: ~5 GB

### llava-phi3 (ожидаемо)

- Загрузка: ~10 секунд
- Ответ: 30-60 секунд
- Память: ~3 GB

## Рекомендация

Для вашей задачи (распознавание измерительных приборов) рекомендую:

1. **minicpm-v:8b** - лучший баланс скорости и качества
2. **llava-phi3** - если нужна максимальная скорость

## Установка и тест

```bash
# Установить minicpm-v
ollama pull minicpm-v:8b

# Изменить в .env
echo "VITE_OLLAMA_VISION_MODEL=minicpm-v:8b" >> .env.local

# Перезапустить приложение
npm run dev
```

## Проверка работы

```bash
# Тест напрямую через Ollama
curl -X POST http://127.0.0.1:11434/api/chat \
  -d '{
    "model": "minicpm-v:8b",
    "messages": [{
      "role": "user",
      "content": "Что на картинке?",
      "images": ["'$(base64 -i image.png)'"]
    }],
    "stream": false
  }'
```
