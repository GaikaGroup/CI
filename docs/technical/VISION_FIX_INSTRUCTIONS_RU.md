# Исправление: Ollama теперь использует LLaVA для изображений

## Что было исправлено

Теперь при загрузке изображения система автоматически использует модель **llava:7b** (vision model) вместо текстовой модели qwen2.5:7b.

## Как проверить, что работает

1. **Запустите приложение**:

   ```bash
   npm run dev
   ```

2. **Откройте чат и загрузите изображение**

3. **Задайте вопрос об изображении**, например:
   - "Что на этой картинке?"
   - "Опиши это изображение"
   - "Какой текст на фото?"

4. **Проверьте логи** в терминале:

   ```
   [Ollama] Using vision model: llava:7b
   ```

   Вместо старого:

   ```
   [Ollama] resolved model: qwen2.5:7b
   ```

## Требования

Убедитесь, что модель llava установлена в Ollama:

```bash
# Проверить установленные модели
ollama list

# Если llava нет, установите:
ollama pull llava:7b
```

## Настройка другой vision модели

Если хотите использовать другую модель (например, llava:13b), создайте файл `.env.local`:

```bash
VITE_OLLAMA_VISION_MODEL=llava:13b
```

Или измените в `.env`:

```bash
VITE_OLLAMA_VISION_MODEL=llava:7b
```

## Что было исправлено технически

Проблема была в обработке языковых напоминаний - система преобразовывала структурированные сообщения (с изображениями) в простой текст, из-за чего терялась информация об изображениях.

Теперь структура сообщений сохраняется правильно:

- ✅ Текстовая часть + языковое напоминание
- ✅ Изображения остаются в структуре
- ✅ Система определяет наличие изображений
- ✅ Автоматически выбирается vision модель

## Поддерживаемые форматы изображений

- PNG
- JPEG/JPG
- WebP
- Base64 encoded images

## Если не работает

1. **Проверьте, что Ollama запущен**:

   ```bash
   curl http://127.0.0.1:11434/api/tags
   ```

2. **Проверьте, что llava установлена**:

   ```bash
   ollama list | grep llava
   ```

3. **Проверьте логи приложения** на наличие ошибок

4. **Перезапустите приложение** после установки llava

## Дополнительная информация

Подробная техническая документация: `VISION_MODEL_FIX.md`
