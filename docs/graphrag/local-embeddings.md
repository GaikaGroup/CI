# Local Embedding Alternatives - Zero Cost Solution

## Overview

Instead of using OpenAI's API ($0.02 per 1M tokens), you can use **local embedding models** that run on your own hardware at **zero cost**.

## Comparison

| Provider                  | Cost            | Quality              | Speed       | Setup Difficulty |
| ------------------------- | --------------- | -------------------- | ----------- | ---------------- |
| **OpenAI**                | $0.02/1M tokens | â­â­â­â­â­ Excellent | âš¡âš¡âš¡ Fast | âœ… Easy          |
| **Sentence-Transformers** | $0              | â­â­â­â­ Very Good   | âš¡âš¡ Medium | ðŸ”§ Medium        |
| **Ollama**                | $0              | â­â­â­â­ Very Good   | âš¡âš¡ Medium | âœ… Easy          |
| **Transformers.js**       | $0              | â­â­â­ Good          | âš¡ Slow     | ðŸ”§ Medium        |

## Option 1: Sentence-Transformers (Recommended)

### Pros

- âœ… **Free** - No API costs
- âœ… **High quality** - 85-90% of OpenAI quality
- âœ… **Privacy** - Data never leaves your server
- âœ… **Many models** - Choose based on your needs

### Cons

- âŒ Requires Python installation
- âŒ Slower than OpenAI API (but still fast)
- âŒ Requires ~500MB disk space per model

### Installation

```bash
# 1. Install Python dependencies
pip install sentence-transformers torch

# 2. Test installation
python3 -c "from sentence_transformers import SentenceTransformer; print('OK')"
```

### Configuration

```bash
# .env
EMBEDDING_PROVIDER="sentence-transformers"
EMBEDDING_MODEL="all-MiniLM-L6-v2"  # Fast, 384 dimensions
EMBEDDING_DIMENSIONS="384"
```

### Available Models

| Model                       | Dimensions | Speed       | Quality    | Use Case                      |
| --------------------------- | ---------- | ----------- | ---------- | ----------------------------- |
| **all-MiniLM-L6-v2**        | 384        | âš¡âš¡âš¡ Fast | â­â­â­â­   | General purpose (recommended) |
| **all-mpnet-base-v2**       | 768        | âš¡âš¡ Medium | â­â­â­â­â­ | Best quality                  |
| **paraphrase-multilingual** | 384        | âš¡âš¡ Medium | â­â­â­â­   | Multiple languages            |
| **all-MiniLM-L12-v2**       | 384        | âš¡âš¡ Medium | â­â­â­â­   | Better quality than L6        |

### Performance

**Your scenario (10 manuals, 3,000 pages):**

- **Cost**: $0 (free!)
- **Processing time**: ~30-60 minutes (one-time)
- **Quality**: 85-90% of OpenAI
- **Disk space**: ~500MB

### Example Usage

```javascript
// Automatic - just set environment variables
const service = new GraphRAGService();
await service.processDocument(content, metadata);

// The system automatically uses sentence-transformers
```

---

## Option 2: Ollama (Easiest Setup)

### Pros

- âœ… **Free** - No API costs
- âœ… **Easy setup** - One command install
- âœ… **Good quality** - Similar to sentence-transformers
- âœ… **REST API** - No Python needed

### Cons

- âŒ Requires Ollama installation
- âŒ Uses more RAM (~2GB)

### Installation

```bash
# 1. Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. Pull embedding model
ollama pull nomic-embed-text

# 3. Verify
curl http://localhost:11434/api/embeddings \
  -d '{"model":"nomic-embed-text","prompt":"test"}'
```

### Configuration

```bash
# .env
EMBEDDING_PROVIDER="ollama"
EMBEDDING_MODEL="nomic-embed-text"
EMBEDDING_DIMENSIONS="768"
OLLAMA_HOST="http://localhost:11434"
```

### Available Models

| Model                 | Dimensions | Quality    | Use Case          |
| --------------------- | ---------- | ---------- | ----------------- |
| **nomic-embed-text**  | 768        | â­â­â­â­   | General purpose   |
| **mxbai-embed-large** | 1024       | â­â­â­â­â­ | Best quality      |
| **all-minilm**        | 384        | â­â­â­     | Fast, lightweight |

### Performance

**Your scenario (10 manuals, 3,000 pages):**

- **Cost**: $0 (free!)
- **Processing time**: ~45-90 minutes
- **Quality**: 85-90% of OpenAI
- **RAM usage**: ~2GB

---

## Option 3: Transformers.js (Browser-Compatible)

### Pros

- âœ… **Free** - No API costs
- âœ… **No Python** - Pure JavaScript
- âœ… **Browser-compatible** - Can run client-side

### Cons

- âŒ Slower than other options
- âŒ Limited model selection
- âŒ Requires additional setup

### Installation

```bash
npm install @xenova/transformers
```

### Configuration

```bash
# .env
EMBEDDING_PROVIDER="transformers-js"
EMBEDDING_MODEL="Xenova/all-MiniLM-L6-v2"
EMBEDDING_DIMENSIONS="384"
```

---

## Cost Comparison for Your Scenario

### 10 Manuals Ã— 300 Pages = 3,000 Pages

| Provider                  | Initial Cost | Monthly Cost | Total Year 1 |
| ------------------------- | ------------ | ------------ | ------------ |
| **OpenAI**                | $18.75       | $0-2         | $18.75-42    |
| **Sentence-Transformers** | $0           | $0           | **$0**       |
| **Ollama**                | $0           | $0           | **$0**       |
| **Transformers.js**       | $0           | $0           | **$0**       |

### Infrastructure Costs

| Provider                  | CPU     | RAM     | Disk    | Monthly Server Cost |
| ------------------------- | ------- | ------- | ------- | ------------------- |
| **OpenAI**                | Minimal | Minimal | Minimal | $5-10               |
| **Sentence-Transformers** | Medium  | 2GB     | 500MB   | $10-20              |
| **Ollama**                | Medium  | 4GB     | 1GB     | $15-25              |

**Note**: Local models require slightly more server resources, but still cheaper than OpenAI for large volumes.

---

## Setup Guide: Sentence-Transformers (Step-by-Step)

### 1. Install Python and Dependencies

```bash
# Check Python version (need 3.8+)
python3 --version

# Install pip if needed
curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
python3 get-pip.py

# Install sentence-transformers
pip3 install sentence-transformers torch
```

### 2. Test Installation

```bash
# Create test script
cat > test_embeddings.py << 'EOF'
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
embedding = model.encode("Hello world")
print(f"Embedding dimensions: {len(embedding)}")
print(f"First 5 values: {embedding[:5]}")
EOF

# Run test
python3 test_embeddings.py
```

**Expected output:**

```
Embedding dimensions: 384
First 5 values: [0.123, -0.456, 0.789, ...]
```

### 3. Configure Application

```bash
# Update .env
cat >> .env << 'EOF'

# Local Embeddings Configuration
EMBEDDING_PROVIDER="sentence-transformers"
EMBEDDING_MODEL="all-MiniLM-L6-v2"
EMBEDDING_DIMENSIONS="384"
PYTHON_PATH="python3"
EOF
```

### 4. Update Database Schema

```bash
# If using 384 dimensions instead of 1536
# Update Prisma schema
```

Edit `prisma/schema.prisma`:

```prisma
model KnowledgeGraphNode {
  // Change from vector(1536) to vector(384)
  embedding  Unsupported("vector(384)")?
}
```

```bash
# Create migration
npx prisma migrate dev --name update_embedding_dimensions

# Generate client
npx prisma generate
```

### 5. Test End-to-End

```bash
# Start application
npm run dev

# Test health check
curl http://localhost:3000/api/health/graphrag

# Should show:
# "embedding": { "status": "ok", "provider": "sentence-transformers" }
```

### 6. Process Documents

```javascript
// Same API as before!
const result = await graphRAGService.processDocument(content, {
  materialId: 'manual-1',
  courseId: 'course-123'
});

// Now using local embeddings (free!)
```

---

## Setup Guide: Ollama (Step-by-Step)

### 1. Install Ollama

```bash
# macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# Or download from https://ollama.com/download
```

### 2. Pull Embedding Model

```bash
# Download model (~500MB)
ollama pull nomic-embed-text

# Verify
ollama list
```

### 3. Test Ollama

```bash
# Test embedding generation
curl http://localhost:11434/api/embeddings \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Hello world"
  }' | jq '.embedding | length'

# Should output: 768
```

### 4. Configure Application

```bash
# Update .env
cat >> .env << 'EOF'

# Ollama Configuration
EMBEDDING_PROVIDER="ollama"
EMBEDDING_MODEL="nomic-embed-text"
EMBEDDING_DIMENSIONS="768"
OLLAMA_HOST="http://localhost:11434"
EOF
```

### 5. Update Database Schema (if needed)

```prisma
// Change to vector(768) for Ollama
embedding  Unsupported("vector(768)")?
```

### 6. Start and Test

```bash
# Ensure Ollama is running
ollama serve

# Start application
npm run dev

# Test
curl http://localhost:3000/api/health/graphrag
```

---

## Performance Benchmarks

### Processing Speed (3,000 pages)

| Provider                        | Time   | Pages/Second |
| ------------------------------- | ------ | ------------ |
| **OpenAI**                      | 15 min | 3.3          |
| **Sentence-Transformers (CPU)** | 45 min | 1.1          |
| **Sentence-Transformers (GPU)** | 10 min | 5.0          |
| **Ollama**                      | 60 min | 0.8          |

### Search Speed (per query)

| Provider          | Latency |
| ----------------- | ------- |
| **All providers** | <200ms  |

_Note: Search speed is the same for all providers because it uses pgvector, not the embedding service._

---

## Hybrid Approach (Best of Both Worlds)

Use **OpenAI for initial processing**, then **local models for updates**:

```bash
# Initial processing (one-time)
EMBEDDING_PROVIDER="openai"
npm run process-initial-documents
# Cost: $18.75

# Switch to local for updates
EMBEDDING_PROVIDER="sentence-transformers"
npm run dev
# Cost: $0/month
```

---

## Troubleshooting

### Sentence-Transformers Issues

**Error: "No module named 'sentence_transformers'"**

```bash
pip3 install sentence-transformers torch
```

**Error: "Model download failed"**

```bash
# Pre-download model
python3 -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"
```

**Slow performance**

```bash
# Use GPU if available
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Ollama Issues

**Error: "Connection refused"**

```bash
# Start Ollama
ollama serve

# Or as service
systemctl start ollama
```

**Error: "Model not found"**

```bash
# Pull model
ollama pull nomic-embed-text
```

---

## Recommendation

**For your scenario (10 manuals, 3,000 pages):**

### If you want easiest setup:

â†’ **Use Ollama** (5 minutes setup, $0 cost)

### If you want best quality:

â†’ **Use Sentence-Transformers with all-mpnet-base-v2** (15 minutes setup, $0 cost)

### If you want fastest processing:

â†’ **Use OpenAI** (2 minutes setup, $18.75 one-time cost)

### If you have GPU:

â†’ **Use Sentence-Transformers with GPU** (fastest + free!)

---

## Migration from OpenAI to Local

Already using OpenAI? Switch to local:

```bash
# 1. Update configuration
EMBEDDING_PROVIDER="sentence-transformers"

# 2. Restart application
npm run dev

# 3. New documents will use local embeddings
# 4. Old embeddings still work (no re-processing needed)
```

---

## Next Steps

1. Choose your provider (Ollama recommended for easiest setup)
2. Follow setup guide above
3. Update `.env` configuration
4. Test with health check
5. Process your documents at $0 cost!

Need help? Check [troubleshooting.md](./troubleshooting.md) or ask for assistance.
