# Руководство по локальным математическим моделям

## Обзор

Это руководство поможет выбрать и настроить локальную математическую модель для AI-тьютора.

## Проблема с Qwen2.5:7b

**Qwen2.5:7b** - это общая языковая модель, **не специализированная на математике**. Она может:

- ✅ Понимать математические термины
- ✅ Генерировать текст о математике
- ❌ Решать сложные математические задачи точно
- ❌ Выполнять многошаговые вычисления без ошибок

**Результат:** Модель может дать неправильный ответ, как в вашем случае.

## Рекомендуемые локальные модели

### 1. Qwen2.5-Math:7b ⭐ РЕКОМЕНДУЕТСЯ

**Специализация:** Математика (алгебра, геометрия, анализ)

**Характеристики:**

- Размер: ~4.7 GB
- RAM: 8+ GB
- Скорость: 15-25 секунд на задачу
- Точность: ~85-90% на стандартных задачах

**Установка:**

```bash
ollama pull qwen2.5-math:7b
```

**Настройка в .env:**

```bash
VITE_MATH_ENABLE_LOCAL_MODELS=true
VITE_OLLAMA_MATH_MODEL=qwen2.5-math:7b
```

**Преимущества:**

- ✅ Специально обучена на математических задачах
- ✅ Хорошо работает с алгеброй и геометрией
- ✅ Показывает пошаговые решения
- ✅ Умеренные требования к ресурсам

**Недостатки:**

- ⚠️ Может ошибаться на очень сложных задачах
- ⚠️ Медленнее GPT-4 (15-25 сек vs 5-10 сек)

### 2. DeepSeek-Math:7b

**Специализация:** Математика (особенно анализ и доказательства)

**Характеристики:**

- Размер: ~4.1 GB
- RAM: 8+ GB
- Скорость: 20-30 секунд на задачу
- Точность: ~80-85% на стандартных задачах

**Установка:**

```bash
ollama pull deepseek-math:7b
```

**Настройка в .env:**

```bash
VITE_MATH_ENABLE_LOCAL_MODELS=true
VITE_OLLAMA_MATH_MODEL=deepseek-math:7b
```

**Преимущества:**

- ✅ Хороша для математического анализа
- ✅ Сильна в доказательствах
- ✅ Меньший размер модели

**Недостатки:**

- ⚠️ Медленнее Qwen2.5-Math
- ⚠️ Может быть менее точной на простых задачах

### 3. Qwen2.5-Math:1.5b (для слабых машин)

**Специализация:** Математика (базовая)

**Характеристики:**

- Размер: ~1.0 GB
- RAM: 4+ GB
- Скорость: 5-10 секунд на задачу
- Точность: ~70-75% на простых задачах

**Установка:**

```bash
ollama pull qwen2.5-math:1.5b
```

**Преимущества:**

- ✅ Очень быстрая
- ✅ Минимальные требования к ресурсам
- ✅ Подходит для простых задач

**Недостатки:**

- ❌ Низкая точность на сложных задачах
- ❌ Ограниченные возможности рассуждения

## Сравнительная таблица

| Модель                   | Размер | RAM   | Скорость  | Точность | Рекомендация            |
| ------------------------ | ------ | ----- | --------- | -------- | ----------------------- |
| **GPT-4-turbo** (OpenAI) | Cloud  | -     | 5-10 сек  | 95%+     | ⭐⭐⭐ Лучшее качество  |
| **Qwen2.5-Math:7b**      | 4.7 GB | 8+ GB | 15-25 сек | 85-90%   | ⭐⭐⭐ Лучший локальный |
| **DeepSeek-Math:7b**     | 4.1 GB | 8+ GB | 20-30 сек | 80-85%   | ⭐⭐ Альтернатива       |
| **Qwen2.5-Math:1.5b**    | 1.0 GB | 4+ GB | 5-10 сек  | 70-75%   | ⭐ Для слабых машин     |
| Qwen2.5:7b (общая)       | 4.7 GB | 8+ GB | 10-15 сек | 50-60%   | ❌ Не для математики    |

## Рекомендации по выбору

### Вариант 1: Максимальное качество (рекомендуется)

```bash
# Используйте GPT-4 для математики
VITE_MATH_ENABLE_LOCAL_MODELS=false
VITE_MATH_MODEL=gpt-4-turbo
```

**Когда использовать:**

- Нужна максимальная точность
- Готовы платить за API (~$0.10 за задачу)
- Важна скорость ответа

### Вариант 2: Баланс качества и стоимости ⭐

```bash
# Локальная модель + fallback на GPT-4
VITE_MATH_ENABLE_LOCAL_MODELS=true
VITE_OLLAMA_MATH_MODEL=qwen2.5-math:7b
VITE_MATH_LOCAL_FALLBACK=true
```

**Когда использовать:**

- Хотите экономить на API
- Есть 8+ GB RAM
- Готовы ждать 15-25 секунд
- Нужен fallback на сложных задачах

### Вариант 3: Полностью локально

```bash
# Только локальная модель
VITE_MATH_ENABLE_LOCAL_MODELS=true
VITE_OLLAMA_MATH_MODEL=qwen2.5-math:7b
VITE_MATH_LOCAL_FALLBACK=false
```

**Когда использовать:**

- Нет доступа к OpenAI API
- Работаете офлайн
- Приватность критична

### Вариант 4: Для слабых машин

```bash
# Легкая модель
VITE_MATH_ENABLE_LOCAL_MODELS=true
VITE_OLLAMA_MATH_MODEL=qwen2.5-math:1.5b
VITE_MATH_LOCAL_FALLBACK=true
```

**Когда использовать:**

- Только 4-6 GB RAM
- Нужна быстрая работа
- Решаете простые задачи

## Настройка гибридного режима (рекомендуется)

Лучший подход - использовать локальную модель с fallback на GPT-4:

```bash
# .env
VITE_MATH_ENABLE_LOCAL_MODELS=true
VITE_OLLAMA_MATH_MODEL=qwen2.5-math:7b
VITE_MATH_LOCAL_FALLBACK=true
VITE_MATH_LOCAL_TIMEOUT=30000

# Если локальная модель не справляется или timeout, используется GPT-4
VITE_ENABLE_LLM_FALLBACK=true
```

**Преимущества:**

- ✅ Экономия на простых задачах (локально)
- ✅ Качество на сложных задачах (GPT-4)
- ✅ Автоматическое переключение

## Проверка установленных моделей

```bash
# Список всех моделей
ollama list

# Проверка конкретной модели
ollama show qwen2.5-math:7b

# Тест модели
ollama run qwen2.5-math:7b "Реши уравнение: 2x + 5 = 13"
```

## Мониторинг производительности

После настройки проверьте логи:

```javascript
// В консоли браузера должны быть логи:
[ProviderManager] Query classification: { isMath: true, confidence: 0.9, category: 'algebra' }
[ProviderManager] Enhanced options for math query: { provider: 'ollama', model: 'qwen2.5-math:7b' }
[ProviderManager] Math query recorded: { category: 'algebra', responseTime: '18500ms' }
```

## Troubleshooting

### Проблема: Модель дает неправильные ответы

**Решение 1:** Переключитесь на GPT-4

```bash
VITE_MATH_ENABLE_LOCAL_MODELS=false
```

**Решение 2:** Включите fallback

```bash
VITE_MATH_LOCAL_FALLBACK=true
```

**Решение 3:** Попробуйте другую модель

```bash
VITE_OLLAMA_MATH_MODEL=deepseek-math:7b
```

### Проблема: Модель слишком медленная

**Решение 1:** Уменьшите контекст

```bash
VITE_OLLAMA_NUM_CTX=2048
```

**Решение 2:** Используйте меньшую модель

```bash
VITE_OLLAMA_MATH_MODEL=qwen2.5-math:1.5b
```

**Решение 3:** Установите timeout

```bash
VITE_MATH_LOCAL_TIMEOUT=20000  # 20 секунд
```

### Проблема: Модель не найдена

```bash
# Проверьте установку
ollama list

# Установите модель
ollama pull qwen2.5-math:7b

# Перезапустите Ollama
ollama serve
```

## Оптимизация для разных типов задач

### Простая арифметика

```bash
# Можно использовать легкую модель
VITE_MATH_ARITHMETIC_MODEL=qwen2.5-math:1.5b
```

### Сложный анализ

```bash
# Лучше использовать GPT-4
VITE_MATH_CALCULUS_MODEL=gpt-4-turbo
VITE_MATH_CALCULUS_PROVIDER=openai
```

## Итоговая рекомендация

**Для вашего случая рекомендую:**

1. **Установите специализированную модель:**

```bash
ollama pull qwen2.5-math:7b
```

2. **Настройте .env:**

```bash
# Используйте локальную математическую модель с fallback
VITE_MATH_ENABLE_LOCAL_MODELS=true
VITE_OLLAMA_MATH_MODEL=qwen2.5-math:7b
VITE_MATH_LOCAL_FALLBACK=true
VITE_ENABLE_LLM_FALLBACK=true
```

3. **Или используйте GPT-4 для гарантированного качества:**

```bash
# Отключите локальные модели для математики
VITE_MATH_ENABLE_LOCAL_MODELS=false
VITE_MATH_MODEL=gpt-4-turbo
```

**Почему это решит проблему:**

- ✅ Qwen2.5-Math:7b специально обучена на математике
- ✅ Fallback на GPT-4 для сложных задач
- ✅ Автоматическое переключение провайдера
- ✅ Правильные ответы на математические задачи

## Дополнительные ресурсы

- [Ollama Models](https://ollama.com/library)
- [Qwen2.5-Math Paper](https://arxiv.org/abs/2409.12122)
- [DeepSeek-Math Paper](https://arxiv.org/abs/2402.03300)
- [Math Enhancement Documentation](./math-reasoning-enhancement.md)
